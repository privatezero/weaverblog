<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Andrew Weaver's Blog</title>
    <description>Welcome to my blog! Sometimes I write about stuff here. And things.
</description>
    <link>privatezero.github.io/weaverblog/</link>
    <atom:link href="privatezero.github.io/weaverblog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 19 Apr 2017 20:18:26 +0000</pubDate>
    <lastBuildDate>Wed, 19 Apr 2017 20:18:26 +0000</lastBuildDate>
    <generator>Jekyll v3.2.1</generator>
    
      <item>
        <title>Adventures In Perceptual Hashing</title>
        <description>&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;
&lt;p&gt;One of the primary goals of my project at CUNY TV is to prototype a system of perceptual hashing that can be integrated into our archival workflows. Perceptual hashing is a method of identifying similar content using automated analysis; the goal being to eliminate the (often impossible) necessity of having a person look at every item one-by-one to make comparisons. Perceptual hashes function in a similar sense to standard checksums, except instead of comparing hashes to establish exact matches between files at the bit level, they establish similarity of content as would be perceived by a viewer or listener.&lt;/p&gt;

&lt;p&gt;There are many examples of perceptual hashing in use that you might already have encountered. If you have ever used Shazam to identify a song, then you have used perceptual hashing! Likewise, if you have done a reverse Google image search, that was also facilitated through perceptual hashing. When you encounter a video on Youtube that has had its audio track muted due to copyright violation, it was probably detected via perceptual hashing.&lt;/p&gt;

&lt;p&gt;One of the key differences between normal checksum comparisons and perceptual hashing is that perceptual hashes attempt to enable linking of original items and derivative or modified versions. For example, if you have a high quality FLAC file of a song and make a copy transcoded to MP3, you will not be able to generate matching checksums from the two files. Likewise, if you added a watermark to a digital video it would be impossible to compare the files using checksums. In both of these examples, even though the actual content is almost identical in a perceptive sense, as data it is completely different.&lt;/p&gt;

&lt;p&gt;Perceptual hashing methods seek to accomodate for these types of changes by applying various transformations to the content before generating the actual hash, also known as a ‘fingerprint’. For example, the audio fingerprinting library &lt;em&gt;Chromaprint&lt;/em&gt; converts all inputs to a sample rate of 11025 Hz before generating representations of the content in musical notes via frequency analysis which are used to create the final fingerprint. &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; The fingerprint standard in MPEG-7 that I have been experimenting with for my project does something analogous, generating global fingerprints per frame with both original and square aspect ratios as well as several sub-fingerprints. &lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; This allows comparisons to be made that are resistant to differences caused by factors such as lossy codecs and cropping.&lt;/p&gt;

&lt;h3 id=&quot;hands-on-example&quot;&gt;Hands on Example&lt;/h3&gt;
&lt;p&gt;As of the version 3.3 release, the powerful Open Source tool FFmpeg has the ability to generate and compare MPEG-7 video fingerprints.  What this means, is that if you have the most recent version of FFmpeg, you are already capable of conducting perceptual hash comparisons! If you don’t have FFmpeg and are interested in installing it, there are excellent instructions for Apple, Linux and Windows users available at &lt;a href=&quot;https://avpres.net/FFmpeg/#ch1&quot;&gt;Reto Kromer’s Webpage&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For this example I used the following video from the University of Washington Libraries’ &lt;a href=&quot;https://archive.org/details/uwlibraries&quot;&gt;Internet Archive  page&lt;/a&gt; as a source.&lt;/p&gt;

&lt;iframe src=&quot;https://archive.org/embed/NarrowsclipH.264ForVideoPodcasting&quot; width=&quot;320&quot; height=&quot;240&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;true&quot; mozallowfullscreen=&quot;true&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;From this source, I created a GIF out of a small excerpt and uploaded it to Github.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/privatezero/Blog-Materials/master/Tacoma.gif&quot; alt=&quot;GIF&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Since FFmpeg supports URLs as inputs, it is possible to test out the filter without downloading the sample files! Just copy the following command into your terminal and try running it! (Sometimes this might time out. In that case just wait a bit and try again.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sample Fingerprint Command&lt;/strong&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;ffmpeg -i https://ia601302.us.archive.org/25/items/NarrowsclipH.264ForVideoPodcasting/Narrowsclip-h.264.ogv  -i https://raw.githubusercontent.com/privatezero/Blog-Materials/master/Tacoma.gif -filter_complex &lt;span class=&quot;nv&quot;&gt;signature&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;detectmode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;full:nb_inputs&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2 -f null -&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Generated Output&lt;/strong&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Parsed_signature_0 @ 0x7feef8c34bc0] matching of video 0 at 80.747414 and 1 at 0.000000, 48 frames matching
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Parsed_signature_0 @ 0x7feef8c34bc0] whole video matching&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;What these results show is that even though the GIF was of lower quality and different dimensions, the FFmpeg signature filter was able to detect that it matched content in the original video starting around 80 seconds in!&lt;/p&gt;

&lt;p&gt;For some further breakdown of how this command works (and lots of other FFmpeg commands), see the example at &lt;a href=&quot;https://amiaopensource.github.io/ffmprovisr/#compare_video_fingerprints&quot;&gt;&lt;strong&gt;ffmprovisr&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;implementing-perceptual-hashing-at-cuny-tv&quot;&gt;Implementing Perceptual Hashing at CUNY TV&lt;/h3&gt;
&lt;p&gt;At CUNY we are interested in perceptual hashing to help identify redundant material, as well as establish connections between shows utilizing identical source content. For example, by implementing systemwide perceptual hashing, it would be possible to take footage from a particular shoot and programmatically search for every production that it had been used in. This would obviously be MUCH faster than viewing videos one by one looking for similar scenes.&lt;/p&gt;

&lt;p&gt;As our goal is collection wide searches, three elements had to be added to existing preservation structures: A method for generating fingerprints on file ingest, a database for storing them and a way to compare files against that database. Fortunately, one of these was already essentially in place. As other part of my residency called for building a database to store other forms of preservation metadata, I had an existing database that I could modify with an additional table for perceptual hashes. The MPEG-7 system of hash comparisons uses a three tiered approach to establish accurate links, starting with a ‘rough fingerprint’ and then moving on to more granular levels.  For simplicity and speed (and due to us not needing accuracy down to fractions of seconds) I decided to only store the components of the ‘rough fingerprint’ in the database.&lt;/p&gt;

&lt;p&gt;As digital preservation at CUNY TV revolves around a set of microservices, I was able to write a script for fingerprint generation and database insertion that can be run on individual files as well as inserted as necessary into preservation actions (such as AIP generation). This script is available &lt;a href=&quot;https://github.com/mediamicroservices/mm/blob/master/makefingerprint&quot;&gt;here&lt;/a&gt; at the mediamicroservices repository on github. Likewise, my script for generating a hash from an input and comparing it against values stored in the database can be found &lt;a href=&quot;https://github.com/mediamicroservices/mm/blob/master/searchfingerprint&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Lalinský, L. (2011, January 18). How Does Chromaprint Work? &lt;a href=&quot;https://oxygene.sk/2011/01/how-does-chromaprint-work/&quot;&gt;https://oxygene.sk/2011/01/how-does-chromaprint-work/&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.worldcat.org/oclc/902763394&quot;&gt;Chiariglione, L. (2014). Mpeg representation of digital media. Springer, 84-85.&lt;/a&gt; &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Tue, 18 Apr 2017 00:00:00 +0000</pubDate>
        <link>privatezero.github.io/weaverblog/2017/04/18/Adventures-in-perceptual-hashing.html</link>
        <guid isPermaLink="true">privatezero.github.io/weaverblog/2017/04/18/Adventures-in-perceptual-hashing.html</guid>
        
        
      </item>
    
      <item>
        <title>Project 'audiorecorder'</title>
        <description>&lt;p&gt;One of the incredible things about my NDSR residency is that it requires me to spend 20% of my time working on professional development.  This allows me to devote quite a bit of time to learning skills and working on things of interest that would otherwise be outside the scope of my project.  (For people who are considering applying to future NDSR cohorts, I can’t emphasize enough how great it is to have the equivalent of one day a week to focus your attention on growth in areas of your choosing).&lt;/p&gt;

&lt;p&gt;A great way to spend some of this time was suggested to me through some personal audio projects I am working on.  I realized that, A: I wanted a tool that would allow me a range of signal monitoring capabilities while digitizing audio. B: I didn’t feel like paying for it. Solution: Try to build one myself!&lt;/p&gt;

&lt;p&gt;As I was familiar with using &lt;a href=&quot;https://github.com/amiaopensource/vrecord&quot;&gt;vrecord&lt;/a&gt; for video digitization, I decided to build a tool called ‘audiorecorder’ that followed the same approach- namely using a shell script to pipe data between FFmpeg and ffplay.  This allows capturing data in high quality, while simultaneously visualizing it.  In looking at the filters available through FFmpeg, I decided to build an interface that would allow monitoring of peak levels, spectrum and phase.  This seemed doable and would give me pretty much everything I would need for both deck calibration and QC while digitizing.  The layout I settled on looks like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/privatezero/Blog-Materials/raw/master/audiorecorder.gif&quot; alt=&quot;audiorecorder interface&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I also wanted audiorecorder to support embedding BWF metadata and to bypass any requirement for opening the file in a traditional DAW for trimming of silence.  After quite a bit of experimentation, I have hit upon a series of looped functions using FFmpeg that allow for files to be manually trimmed both at the start and at the end, with the option for auto silence detect at the start.  Being able to do this type of manipulation makes it possible to embed BWF metadata without worrying about having to re-embed in an edited file.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/privatezero/Blog-Materials/raw/master/post_trim.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To test the robustness of digitization quality I used the global analysis tools in the free trial version of Wavelab.  After some hiccups early in the process I have been pleased to find that my tool is performing reliably with no dropped samples! (Right now it is using a pretty large buffer. This contributes to latency, especialy when only capturing one channel. One of my next steps will be to test how far I can roll that back while adding variable buffering depending on capture choices).&lt;/p&gt;

&lt;p&gt;Overall this has been a very fun process, and has accomplished the goal of ‘Professional Development’ in several ways.  I have been able to gain more experience both with scripting and with manipulating media streams.  Since the project is based on the AMIA Open Source Github site, I have been able to learn more about managing collaborative projects in an open-source context.  While initially intimidating, it has been exciting to work on a tool in public and benefit from constant feedback and support.  Plus, at the end of the day I am left with a free and open tool that fulfills my audio archiving needs!&lt;/p&gt;

&lt;p&gt;Audiorecorder is based &lt;a href=&quot;https://github.com/amiaopensource/audiorecorder&quot;&gt;here&lt;/a&gt;, and is installable via homebrew.  Some of the next steps I am working on are to get a bit of real usage documentation, and to ensure that it is also possible to install audiorecorder via linuxbrew.&lt;/p&gt;

&lt;p&gt;I would like to give particular thanks to the contributors who have helped with the project so far, Reto Kromer, Matt Boyd and Dave Rice!&lt;/p&gt;
</description>
        <pubDate>Mon, 27 Mar 2017 00:00:00 +0000</pubDate>
        <link>privatezero.github.io/weaverblog/2017/03/27/Project-'audiorecorder'.html</link>
        <guid isPermaLink="true">privatezero.github.io/weaverblog/2017/03/27/Project-'audiorecorder'.html</guid>
        
        
      </item>
    
      <item>
        <title>Learning Github</title>
        <description>&lt;h1 id=&quot;learning-github-or-if-i-can-do-it-you-can-too&quot;&gt;Learning GitHub (or, if I can do it, you can too!)&lt;/h1&gt;

&lt;p&gt;As the CUNY TV Archives makes heavy use of GitHub for their workflows, in my residency I have had the opportunity to learn about more efficient ways to use this tool.  GitHub is a service that not only allows for the hosting of information, but for tracking of changes to that information and collaboration.  Before I started my residency I took advantage of GitHub to store and distribute scripts across different units in the Library.  This was great, however, the way in which I was using it (cut and pasting into the browser window and updating without any comments) made it impossible to take advantage of a lot of GitHub’s functionality, along with creating lots of headaches. Enter GitHub Desktop.&lt;/p&gt;

&lt;p&gt;GitHub desktop is a GUI (Graphical User Interface) that enables you to control and keep track of activity on your GitHub repositories.  I had actually experimented with it when I first started GitHub, but stopped because I found it confusing.  This is because I mistakenly believed that it was used as an editor rather than a means of tracking and controlling edits, an important distinction.  Well, enough talking - more rocking!  Let’s take a quick look at how to get started using GitHub desktop!&lt;/p&gt;

&lt;p&gt;The first thing you need is, not surprisingly, a &lt;a href=&quot;https://github.com/&quot;&gt;GitHub account&lt;/a&gt;!  In creating your account, there are many resources available online to help you pick your &lt;a href=&quot;http://www.ghostlightning.com/hacker.html&quot;&gt;hacker name&lt;/a&gt; (this step is VERY important). Next, install &lt;a href=&quot;https://desktop.github.com/&quot;&gt;GitHub Desktop&lt;/a&gt; and sign in with your account.&lt;/p&gt;

&lt;p&gt;Once you are signed in, try creating a repository (a place where your projects will live) by going to the ‘Repositories’ tab and clicking ‘New’ in the upper left corner.  Now you can choose a name and description for your repository.  Make sure to check the box for ‘Initialize this Repository with a README’ as this is what we will be editing in this example.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/privatezero/Blog-Materials/master/Github_Blog_Pic1.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once you have created your Repository, you can ‘Clone’ it to your computer and manage it with the GitHub desktop application. Do this by clicking the ‘Clone or Download’ button and selecting ‘Open in Desktop.’ Answer yes to the prompts and GitHub Desktop will load. It will ask you where you would like to download the repository; you can use the default or pick somewhere else.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/privatezero/Blog-Materials/master/Github_Blog_Pic2.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once you have the repository cloned to your computer, you can start working on it and managing changes!  Under the ‘Repository’ menu, choose ‘Open in Finder.’  This should open a folder (your repository!) that contains a file called README.md.  Open this file in a text editor.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/privatezero/Blog-Materials/master/Github_Blog_Pic3.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the text editor you should see a little bit of information already, such as the name of your repository.  Try adding something and saving the file!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/privatezero/Blog-Materials/master/Github_Blog_Pic4.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once you have done this, go back to GitHub Desktop.  At the top of the window there should be a button that says ‘1 Uncommitted Change.’ Click on this, and it will bring up a screen that allows you to accept/reject and describe changes made to your file.  By default all of the changes are flagged to be accepted.  Click on the blue area to deselect them if you wish.  Then add a name/description of the change and hit ‘Commit to Master.’  Press the ‘Sync’ button in the upper right corner, and now the changes have been logged and added to your online repository!  Try refreshing your repository in your browser and see what it looks like-your changes should be there!  Additionally, your change has been recorded in the history of your repository.  This will become super useful once you start building something incrementally (or with multiple people).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/privatezero/Blog-Materials/master/Github_Blog_Pic5.png&quot; width=&quot;650&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Obviously this just scratches the surface of how to use GitHub, and there are other ways to manage your repository (such as via the command line) but hopefully it can be a first step towards feeling more comfortable with this tool.  It’s hacking time!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/privatezero/Blog-Materials/master/hackerman.gif&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Thu, 13 Oct 2016 00:00:00 +0000</pubDate>
        <link>privatezero.github.io/weaverblog/2016/10/13/Learning-Github.html</link>
        <guid isPermaLink="true">privatezero.github.io/weaverblog/2016/10/13/Learning-Github.html</guid>
        
        
      </item>
    
  </channel>
</rss>
